{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "from src.core import file_manager  as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start(spark32=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+\n",
      "|text                                            |\n",
      "+------------------------------------------------+\n",
      "|Peter Parker is a nice guy and lives in New York|\n",
      "+------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = 'Peter Parker is a nice guy and lives in New York'\n",
    "\n",
    "spark_df = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "\n",
    "spark_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documentAssembler = DocumentAssembler().setInputCol(\"txt\").setOutputCol(\"document\")\n",
    "\n",
    "# sentence_detector = SentenceDetector().setInputCols([\"document\"]).setOutputCol(\"sentence\")\n",
    "\n",
    "# tokenizer = Tokenizer().setInputCols([\"sentence\"]).setOutputCol(\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'Peter Parker (Spiderman) is a nice guy and lives in New York but has no e-mail!'\n",
    "def get_tokens(text):\n",
    "    spark_df = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "\n",
    "    doc_df = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\").transform(spark_df)\n",
    "\n",
    "    token_df = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\").fit(doc_df).transform(doc_df)\n",
    "\n",
    "    return token_df.select('token.result').toPandas().loc[0]['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eu', 'te', 'amo', ',', 'gata', '!']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tokens('Eu te amo, gata!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_6B_300 download started this may take some time.\n",
      "Approximate size to download 426,2 MB\n",
      "[ | ]glove_6B_300 download started this may take some time.\n",
      "Approximate size to download 426,2 MB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler().setInputCol(\"txt\").setOutputCol(\"document\")\n",
    "\n",
    "sentence_detector = SentenceDetector().setInputCols([\"document\"]).setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer().setInputCols([\"sentence\"]).setOutputCol(\"token\")\n",
    "\n",
    "word_embedding = WordEmbeddingsModel().pretrained(\"glove_6B_300\", \"xx\") \\\n",
    "    .setInputCols(\"document\", \"token\") \\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "embeddings_sentenses = SentenceEmbeddings().setInputCols(['document', 'embeddings']) \\\n",
    "    .setOutputCol('sentence_embeddings').setPoolingStrategy('AVERAGE')\n",
    "\n",
    "stages = [documentAssembler, sentence_detector, tokenizer, word_embedding, embeddings_sentenses]\n",
    "\n",
    "use_clf_pipeline = Pipeline(stages=stages)\n",
    "\n",
    "use_pipeline_model = use_clf_pipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------+\n",
      "|txt                                                           |\n",
      "+--------------------------------------------------------------+\n",
      "|Tô  muito nervosa Marcela com tudo isso                       |\n",
      "|O meu problema  é  psicológico    nem dormi estou  conseguindo|\n",
      "|Com muito medo                                                |\n",
      "|Sim                                                           |\n",
      "|A 3 meses atrás  quando o meu esposo faleceu                  |\n",
      "|Não                                                           |\n",
      "|Tem algum remédio  que a pessoa tome para se acalmar          |\n",
      "|Pra  ficar tranquila                                          |\n",
      "|Tá  certo                                                     |\n",
      "|Tá  bom                                                       |\n",
      "+--------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_annotated_sentences = fm.filename_from_data_dir('output/patient/annotated_sentences.csv')\n",
    "\n",
    "data = spark.read.option(\"header\", True).csv(path_annotated_sentences)\n",
    "\n",
    "data = data.select('txt').limit(10)\n",
    "\n",
    "data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['txt', 'document', 'sentence', 'token', 'embeddings', 'sentence_embeddings']\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                 txt|            document|            sentence|               token|          embeddings| sentence_embeddings|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Tô  muito nervosa...|[{document, 0, 38...|[{document, 0, 38...|[{token, 0, 1, Tô...|[{word_embeddings...|[{sentence_embedd...|\n",
      "|O meu problema  é...|[{document, 0, 61...|[{document, 0, 61...|[{token, 0, 0, O,...|[{word_embeddings...|[{sentence_embedd...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# output_columns = ['txt', 'token', 'embeddings', 'sentence_embeddings']\n",
    "# df_embeddings = use_pipeline_model.transform(self.annotated_sentences).select(output_columns)\n",
    "\n",
    "result = use_pipeline_model.transform(data).limit(2)\n",
    "\n",
    "print(result.columns)\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                 txt|              tokens|     word_embeddings|          embeddings|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|Tô  muito nervosa...|[Tô, muito, nervo...|[[0.14694, -0.524...|[[0.08082773, -0....|\n",
      "|O meu problema  é...|[O, meu, problema...|[[-0.0018332, -0....|[[0.14869732, -0....|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_columns = [\n",
    "  'txt',\n",
    "  result.token.result.alias('tokens'),\n",
    "  result.embeddings.embeddings.alias('word_embeddings'),\n",
    "  'sentence_embeddings.embeddings'\n",
    "]\n",
    "\n",
    "result.select(output_columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_file = fm.filename_from_data_dir('output/glove_spark.json')\n",
    "\n",
    "result.select(output_columns).write.json(dir_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word_embeddings</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tô  muito nervosa Marcela com tudo isso</td>\n",
       "      <td>[Tô, muito, nervosa, Marcela, com, tudo, isso]</td>\n",
       "      <td>[[0.14694000000000002, -0.5241800000000001, -0...</td>\n",
       "      <td>[[0.08082773, -0.035568573, -0.048247125, 0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O meu problema  é  psicológico    nem dormi es...</td>\n",
       "      <td>[O, meu, problema, é, psicológico, nem, dormi,...</td>\n",
       "      <td>[[-0.0018331999999999999, -0.3367, -0.13084, 0...</td>\n",
       "      <td>[[0.14869732, -0.14268143, -0.074482225, 0.045...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 txt  \\\n",
       "0            Tô  muito nervosa Marcela com tudo isso   \n",
       "1  O meu problema  é  psicológico    nem dormi es...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0     [Tô, muito, nervosa, Marcela, com, tudo, isso]   \n",
       "1  [O, meu, problema, é, psicológico, nem, dormi,...   \n",
       "\n",
       "                                     word_embeddings  \\\n",
       "0  [[0.14694000000000002, -0.5241800000000001, -0...   \n",
       "1  [[-0.0018331999999999999, -0.3367, -0.13084, 0...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [[0.08082773, -0.035568573, -0.048247125, 0.06...  \n",
       "1  [[0.14869732, -0.14268143, -0.074482225, 0.045...  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = fm.read_json_of_dir(dir_file, lines=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from src.core import file_manager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import FlairEmbeddings, DocumentPoolEmbeddings, WordEmbeddings, StackedEmbeddings\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import FlairEmbeddings, DocumentPoolEmbeddings, WordEmbeddings, StackedEmbeddings\n",
    "from flair.data import Sentence\n",
    "\n",
    "flair_embedding_forward = FlairEmbeddings('pt-forward')\n",
    "flair_embedding_backward = FlairEmbeddings('pt-backward')\n",
    "document_pool_embeddings = DocumentPoolEmbeddings([flair_embedding_forward, flair_embedding_backward])\n",
    "stacked_embeddings = StackedEmbeddings([flair_embedding_forward, flair_embedding_backward])\n",
    "\n",
    "\n",
    "def get_sentence_embedding(txt):  \n",
    "  sentence = Sentence(str(txt))\n",
    "  \n",
    "  document_pool_embeddings.embed(sentence)\n",
    "  stacked_embeddings.embed(sentence)\n",
    "\n",
    "  data = {\n",
    "    'tokens': [token.text for token in sentence.tokens],\n",
    "    'word_embeddings': [token.embedding.tolist() for token in sentence.tokens],\n",
    "    'embeddings': [sentence.embedding.tolist()]\n",
    "  }\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>annotated_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tô  muito nervosa Marcela com tudo isso</td>\n",
       "      <td>Tô  muito nervosa Marcela com tudo isso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O meu problema  é  psicológico    nem dormi es...</td>\n",
       "      <td>O meu problema  é  psicológico    nem dormi es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Com muito medo</td>\n",
       "      <td>Com muito medo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sim</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A 3 meses atrás  quando o meu esposo faleceu</td>\n",
       "      <td>A 3 meses atrás  quando o meu esposo faleceu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tem algum remédio  que a pessoa tome para se a...</td>\n",
       "      <td>Tem algum remédio  que a pessoa tome para se a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pra  ficar tranquila</td>\n",
       "      <td>Pra  ficar tranquila</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tá  certo</td>\n",
       "      <td>Tá  certo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tá  bom</td>\n",
       "      <td>Tá  bom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 txt  \\\n",
       "0            Tô  muito nervosa Marcela com tudo isso   \n",
       "1  O meu problema  é  psicológico    nem dormi es...   \n",
       "2                                     Com muito medo   \n",
       "3                                                Sim   \n",
       "4       A 3 meses atrás  quando o meu esposo faleceu   \n",
       "5                                                Não   \n",
       "6  Tem algum remédio  que a pessoa tome para se a...   \n",
       "7                               Pra  ficar tranquila   \n",
       "8                                          Tá  certo   \n",
       "9                                            Tá  bom   \n",
       "\n",
       "                                       annotated_txt  \n",
       "0            Tô  muito nervosa Marcela com tudo isso  \n",
       "1  O meu problema  é  psicológico    nem dormi es...  \n",
       "2                                     Com muito medo  \n",
       "3                                                Sim  \n",
       "4       A 3 meses atrás  quando o meu esposo faleceu  \n",
       "5                                                Não  \n",
       "6  Tem algum remédio  que a pessoa tome para se a...  \n",
       "7                               Pra  ficar tranquila  \n",
       "8                                          Tá  certo  \n",
       "9                                            Tá  bom  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotated = pd.read_csv(fm.filename_from_data_dir('output/patient/annotated_sentences.csv'))\n",
    "\n",
    "df_annotated.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>word_embeddings</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Tô, muito, nervosa, Marcela, com, tudo, isso]</td>\n",
       "      <td>[[0.0035118244122713804, -0.005027011502534151...</td>\n",
       "      <td>[[0.0005851125461049378, -0.000787119206506758...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[O, meu, problema, é, psicológico, nem, dormi,...</td>\n",
       "      <td>[[-0.00020597137336153537, -0.0036803667899221...</td>\n",
       "      <td>[[0.0023417221382260323, -0.000689701177179813...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Com, muito, medo]</td>\n",
       "      <td>[[-0.00020977245003450662, -0.0014247068902477...</td>\n",
       "      <td>[[-0.0010116567136719823, -0.00071526312967762...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Sim]</td>\n",
       "      <td>[[-0.02557721920311451, 0.0003712331235874444,...</td>\n",
       "      <td>[[-0.02557721920311451, 0.0003712331235874444,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[A, 3, meses, atrás, quando, o, meu, esposo, f...</td>\n",
       "      <td>[[-0.00011327546963002533, -0.0050820899195969...</td>\n",
       "      <td>[[-0.0020755650475621223, -0.00134362292010337...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0     [Tô, muito, nervosa, Marcela, com, tudo, isso]   \n",
       "1  [O, meu, problema, é, psicológico, nem, dormi,...   \n",
       "2                                 [Com, muito, medo]   \n",
       "3                                              [Sim]   \n",
       "4  [A, 3, meses, atrás, quando, o, meu, esposo, f...   \n",
       "\n",
       "                                     word_embeddings  \\\n",
       "0  [[0.0035118244122713804, -0.005027011502534151...   \n",
       "1  [[-0.00020597137336153537, -0.0036803667899221...   \n",
       "2  [[-0.00020977245003450662, -0.0014247068902477...   \n",
       "3  [[-0.02557721920311451, 0.0003712331235874444,...   \n",
       "4  [[-0.00011327546963002533, -0.0050820899195969...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [[0.0005851125461049378, -0.000787119206506758...  \n",
       "1  [[0.0023417221382260323, -0.000689701177179813...  \n",
       "2  [[-0.0010116567136719823, -0.00071526312967762...  \n",
       "3  [[-0.02557721920311451, 0.0003712331235874444,...  \n",
       "4  [[-0.0020755650475621223, -0.00134362292010337...  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_annotated, df_data], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>annotated_txt</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word_embeddings</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tô  muito nervosa Marcela com tudo isso</td>\n",
       "      <td>Tô  muito nervosa Marcela com tudo isso</td>\n",
       "      <td>[Tô, muito, nervosa, Marcela, com, tudo, isso]</td>\n",
       "      <td>[[0.0035118244122713804, -0.005027011502534151...</td>\n",
       "      <td>[[0.0005851125461049378, -0.000787119206506758...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O meu problema  é  psicológico    nem dormi es...</td>\n",
       "      <td>O meu problema  é  psicológico    nem dormi es...</td>\n",
       "      <td>[O, meu, problema, é, psicológico, nem, dormi,...</td>\n",
       "      <td>[[-0.00020597137336153537, -0.0036803667899221...</td>\n",
       "      <td>[[0.0023417221382260323, -0.000689701177179813...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Com muito medo</td>\n",
       "      <td>Com muito medo</td>\n",
       "      <td>[Com, muito, medo]</td>\n",
       "      <td>[[-0.00020977245003450662, -0.0014247068902477...</td>\n",
       "      <td>[[-0.0010116567136719823, -0.00071526312967762...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sim</td>\n",
       "      <td>Sim</td>\n",
       "      <td>[Sim]</td>\n",
       "      <td>[[-0.02557721920311451, 0.0003712331235874444,...</td>\n",
       "      <td>[[-0.02557721920311451, 0.0003712331235874444,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A 3 meses atrás  quando o meu esposo faleceu</td>\n",
       "      <td>A 3 meses atrás  quando o meu esposo faleceu</td>\n",
       "      <td>[A, 3, meses, atrás, quando, o, meu, esposo, f...</td>\n",
       "      <td>[[-0.00011327546963002533, -0.0050820899195969...</td>\n",
       "      <td>[[-0.0020755650475621223, -0.00134362292010337...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 txt  \\\n",
       "0            Tô  muito nervosa Marcela com tudo isso   \n",
       "1  O meu problema  é  psicológico    nem dormi es...   \n",
       "2                                     Com muito medo   \n",
       "3                                                Sim   \n",
       "4       A 3 meses atrás  quando o meu esposo faleceu   \n",
       "\n",
       "                                       annotated_txt  \\\n",
       "0            Tô  muito nervosa Marcela com tudo isso   \n",
       "1  O meu problema  é  psicológico    nem dormi es...   \n",
       "2                                     Com muito medo   \n",
       "3                                                Sim   \n",
       "4       A 3 meses atrás  quando o meu esposo faleceu   \n",
       "\n",
       "                                              tokens  \\\n",
       "0     [Tô, muito, nervosa, Marcela, com, tudo, isso]   \n",
       "1  [O, meu, problema, é, psicológico, nem, dormi,...   \n",
       "2                                 [Com, muito, medo]   \n",
       "3                                              [Sim]   \n",
       "4  [A, 3, meses, atrás, quando, o, meu, esposo, f...   \n",
       "\n",
       "                                     word_embeddings  \\\n",
       "0  [[0.0035118244122713804, -0.005027011502534151...   \n",
       "1  [[-0.00020597137336153537, -0.0036803667899221...   \n",
       "2  [[-0.00020977245003450662, -0.0014247068902477...   \n",
       "3  [[-0.02557721920311451, 0.0003712331235874444,...   \n",
       "4  [[-0.00011327546963002533, -0.0050820899195969...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [[0.0005851125461049378, -0.000787119206506758...  \n",
       "1  [[0.0023417221382260323, -0.000689701177179813...  \n",
       "2  [[-0.0010116567136719823, -0.00071526312967762...  \n",
       "3  [[-0.02557721920311451, 0.0003712331235874444,...  \n",
       "4  [[-0.0020755650475621223, -0.00134362292010337...  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Estou tossindo, com febre e dor de cabeça, trysdas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = get_sentence_embedding(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r['word_embeddings'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sentence' object has no attribute 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/valmir/dev/python/intent_classifier/notebooks/helper.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/valmir/dev/python/intent_classifier/notebooks/helper.ipynb#ch0000018?line=7'>8</a>\u001b[0m stacked_embeddings\u001b[39m.\u001b[39membed(sentence)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/valmir/dev/python/intent_classifier/notebooks/helper.ipynb#ch0000018?line=10'>11</a>\u001b[0m \u001b[39m# for token in sentence:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/valmir/dev/python/intent_classifier/notebooks/helper.ipynb#ch0000018?line=11'>12</a>\u001b[0m \u001b[39m#     print(token.text,'=', len(token.embedding))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/valmir/dev/python/intent_classifier/notebooks/helper.ipynb#ch0000018?line=12'>13</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/valmir/dev/python/intent_classifier/notebooks/helper.ipynb#ch0000018?line=13'>14</a>\u001b[0m \u001b[39m# sentence.tokens[0].text\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/valmir/dev/python/intent_classifier/notebooks/helper.ipynb#ch0000018?line=15'>16</a>\u001b[0m sentence\u001b[39m.\u001b[39;49mtokenizer\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sentence' object has no attribute 'tokenizer'"
     ]
    }
   ],
   "source": [
    "flair_embedding_forward = FlairEmbeddings('pt-forward')\n",
    "flair_embedding_backward = FlairEmbeddings('pt-backward')\n",
    "\n",
    "stacked_embeddings = StackedEmbeddings([flair_embedding_forward, flair_embedding_backward])\n",
    "\n",
    "sentence = Sentence('Estou com muita febre e dor de cabeça, o que posso fazer para melhorar?')\n",
    "\n",
    "stacked_embeddings.embed(sentence)\n",
    "\n",
    "\n",
    "# for token in sentence:\n",
    "#     print(token.text,'=', len(token.embedding))\n",
    "\n",
    "# sentence.tokens[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Sentence in module flair.data object:\n",
      "\n",
      "class Sentence(DataPoint)\n",
      " |  Sentence(text: Union[str, List[str]], use_tokenizer: Union[bool, flair.data.Tokenizer] = True, language_code: str = None, start_position: int = 0)\n",
      " |  \n",
      " |  A Sentence is a list of tokens and is used to represent a sentence or text fragment.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Sentence\n",
      " |      DataPoint\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __copy__(self)\n",
      " |  \n",
      " |  __getitem__(self, subscript)\n",
      " |  \n",
      " |  __init__(self, text: Union[str, List[str]], use_tokenizer: Union[bool, flair.data.Tokenizer] = True, language_code: str = None, start_position: int = 0)\n",
      " |      Class to hold all meta related to a text (tokens, predictions, language code, ...)\n",
      " |      :param text: original string (sentence), or a list of string tokens (words)\n",
      " |      :param use_tokenizer: a custom tokenizer (default is :class:`SpaceTokenizer`)\n",
      " |          more advanced options are :class:`SegTokTokenizer` to use segtok or :class:`SpacyTokenizer`\n",
      " |          to use Spacy library if available). Check the implementations of abstract class Tokenizer or\n",
      " |          implement your own subclass (if you need it). If instead of providing a Tokenizer, this parameter\n",
      " |          is just set to True (deprecated), :class:`SegtokTokenizer` will be used.\n",
      " |      :param language_code: Language of the sentence\n",
      " |      :param start_position: Start char offset of the sentence in the superordinate document\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_token(self, token: Union[flair.data.Token, str])\n",
      " |  \n",
      " |  clear_embeddings(self, embedding_names: List[str] = None)\n",
      " |  \n",
      " |  get_labels(self, label_type: str = None)\n",
      " |  \n",
      " |  get_language_code(self) -> str\n",
      " |  \n",
      " |  get_relations(self, type: str) -> List[flair.data.Relation]\n",
      " |  \n",
      " |  get_span(self, start: int, stop: int)\n",
      " |  \n",
      " |  get_spans(self, type: str) -> List[flair.data.Span]\n",
      " |  \n",
      " |  get_token(self, token_id: int) -> Optional[flair.data.Token]\n",
      " |  \n",
      " |  infer_space_after(self)\n",
      " |      Heuristics in case you wish to infer whitespace_after values for tokenized text. This is useful for some old NLP\n",
      " |      tasks (such as CoNLL-03 and CoNLL-2000) that provide only tokenized data with no info of original whitespacing.\n",
      " |      :return:\n",
      " |  \n",
      " |  is_context_set(self) -> bool\n",
      " |      Return True or False depending on whether context is set (for instance in dataloader or elsewhere)\n",
      " |      :return: True if context is set, else False\n",
      " |  \n",
      " |  left_context(self, context_length: int, respect_document_boundaries: bool = True)\n",
      " |  \n",
      " |  next_sentence(self)\n",
      " |      Get the next sentence in the document (works only if context is set through dataloader or elsewhere)\n",
      " |      :return: next Sentence in document if set, otherwise None\n",
      " |  \n",
      " |  previous_sentence(self)\n",
      " |      Get the previous sentence in the document (works only if context is set through dataloader or elsewhere)\n",
      " |      :return: previous Sentence in document if set, otherwise None\n",
      " |  \n",
      " |  remove_labels(self, typename: str)\n",
      " |  \n",
      " |  right_context(self, context_length: int, respect_document_boundaries: bool = True)\n",
      " |  \n",
      " |  to(self, device: str, pin_memory: bool = False)\n",
      " |  \n",
      " |  to_dict(self, tag_type: str = None)\n",
      " |  \n",
      " |  to_original_text(self) -> str\n",
      " |  \n",
      " |  to_plain_string(self)\n",
      " |  \n",
      " |  to_tagged_string(self, main_label=None) -> str\n",
      " |  \n",
      " |  to_tokenized_string(self) -> str\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  embedding\n",
      " |  \n",
      " |  end_position\n",
      " |  \n",
      " |  start_position\n",
      " |  \n",
      " |  text\n",
      " |  \n",
      " |  unlabeled_identifier\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from DataPoint:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __lt__(self, other)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  add_label(self, typename: str, value: str, score: float = 1.0)\n",
      " |  \n",
      " |  get_each_embedding(self, embedding_names: Optional[List[str]] = None) -> List[torch.Tensor]\n",
      " |  \n",
      " |  get_embedding(self, names: Optional[List[str]] = None) -> torch.Tensor\n",
      " |  \n",
      " |  get_label(self, label_type: str = None, zero_tag_value='O')\n",
      " |  \n",
      " |  has_label(self, type) -> bool\n",
      " |  \n",
      " |  set_embedding(self, name: str, vector: torch.Tensor)\n",
      " |  \n",
      " |  set_label(self, typename: str, value: str, score: float = 1.0)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from DataPoint:\n",
      " |  \n",
      " |  labels\n",
      " |  \n",
      " |  score\n",
      " |  \n",
      " |  tag\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from DataPoint:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "help(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_sentence_embedding('Eu estou com febre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.core import file_manager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_indexes_dict = {\n",
    "  'greeting': 0,\n",
    "  'inform_medicine': 1,\n",
    "  'inform_symptoms': 2,\n",
    "  'request_inform': 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meu nariz tá entupido né mas não tá me incomod...</td>\n",
       "      <td>inform_symptoms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boa noite, Valéria. Desde sexta que estou com ...</td>\n",
       "      <td>inform_symptoms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>estava espirrando muito</td>\n",
       "      <td>inform_symptoms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dipirona não vai tratar</td>\n",
       "      <td>inform_medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E como a tossi desaparece? Sem tomar remédio?</td>\n",
       "      <td>request_inform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>Ele corre riscos?</td>\n",
       "      <td>request_inform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>Unico sintoma so a tosse</td>\n",
       "      <td>inform_symptoms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>Garganta doendo um pouco</td>\n",
       "      <td>inform_symptoms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>Mas estava apouco com muito frio tomei dipiron...</td>\n",
       "      <td>inform_symptoms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>meu marido está sentindo umas palpitações no c...</td>\n",
       "      <td>inform_symptoms</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>594 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   txt           intent\n",
       "0    Meu nariz tá entupido né mas não tá me incomod...  inform_symptoms\n",
       "1    Boa noite, Valéria. Desde sexta que estou com ...  inform_symptoms\n",
       "2                              estava espirrando muito  inform_symptoms\n",
       "3                              Dipirona não vai tratar  inform_medicine\n",
       "4        E como a tossi desaparece? Sem tomar remédio?   request_inform\n",
       "..                                                 ...              ...\n",
       "589                                  Ele corre riscos?   request_inform\n",
       "590                           Unico sintoma so a tosse  inform_symptoms\n",
       "591                           Garganta doendo um pouco  inform_symptoms\n",
       "592  Mas estava apouco com muito frio tomei dipiron...  inform_symptoms\n",
       "593  meu marido está sentindo umas palpitações no c...  inform_symptoms\n",
       "\n",
       "[594 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = fm.filename_from_data_dir('output/patient/sentences_with_label_manual_without_others.csv')\n",
    "\n",
    "df_data_to_valid = pd.read_csv(path)\n",
    "\n",
    "df_data_to_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acurracy(embedding_name):\n",
    "  df = fm.read_annotated_df_with_embeddings(embedding_name)\n",
    "\n",
    "  X = np.array(df['embeddings'].map(lambda x: np.array(x[0])).to_list())\n",
    "  y = df['intent'].map(intent_indexes_dict).to_numpy()\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "  clf = svm.SVC(max_iter=10000, decision_function_shape='ovo')\n",
    "  clf.fit(X_train, y_train)\n",
    "\n",
    "  predict = clf.predict(X_test)\n",
    "\n",
    "  # lin_clf = svm.LinearSVC()\n",
    "  # lin_clf.fit(X_train, y_train)\n",
    "  # predict = lin_clf.predict(X_test)\n",
    "\n",
    "  correct_predict = np.equal(predict, y_test).sum()\n",
    "\n",
    "  acuracy = correct_predict / len(y_test)\n",
    "\n",
    "  return acuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fm.read_annotated_df_with_embeddings('glove')\n",
    "\n",
    "X = np.array(df['embeddings'].map(lambda x: np.array(x[0])).to_list())\n",
    "y = df['intent'].map(intent_indexes_dict).to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "clf = svm.SVC(max_iter=10000, decision_function_shape='ovo')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict = clf.predict(X_test)\n",
    "\n",
    "# lin_clf = svm.LinearSVC()\n",
    "# lin_clf.fit(X_train, y_train)\n",
    "# predict = lin_clf.predict(X_test)\n",
    "\n",
    "correct_predict = np.equal(predict, y_test).sum()\n",
    "\n",
    "acuracy = correct_predict / len(y_test)\n",
    "\n",
    "acuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_data_to_valid, df, on='txt', how='left')\n",
    "\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9487179487179487"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_acurracy('bert_pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9775280898876404"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_acurracy('glove')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9536199095022625"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_acurracy('flair_pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9858510195588848"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_acurracy('lasbe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9894179894179894"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_acurracy('use')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da386d383b1a4f2ab977ab0cc371ee70ac9bd2160da3f0bf5cca7f94d497a6a3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
