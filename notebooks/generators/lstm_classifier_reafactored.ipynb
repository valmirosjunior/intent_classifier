{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zL6SjRcWyrkQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-16 17:50:11.999340: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-16 17:50:11.999355: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from src.core import file_manager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary_from_df(dataframe):\n",
    "  keyed_vectors = {}\n",
    "\n",
    "  for row_index, row in dataframe.iterrows():\n",
    "    for index, token in enumerate(row['tokens']):\n",
    "      if token not in keyed_vectors:\n",
    "          keyed_vectors[token] = row['word_embeddings'][index]\n",
    "  return Vocabulary(keyed_vectors)\n",
    "\n",
    "      \n",
    "class Vocabulary:\n",
    "  def __init__(self, keyed_vectors):\n",
    "    self.keyed_vectors = keyed_vectors\n",
    "    self.keys_to_index =  self.get_keys_to_index()\n",
    "    self.embedding_matrix = self.build_embedding_matrix()\n",
    "\n",
    "  def get_keys_to_index(self):\n",
    "    return {key: index for index, key in enumerate(self.keyed_vectors.keys())}\n",
    "\n",
    "  def build_embedding_matrix(self):        \n",
    "    first_key_of_keyed_vectors = list(self.keyed_vectors.keys())[0]\n",
    "    emb_size = len(self.keyed_vectors[first_key_of_keyed_vectors])\n",
    "    n_words = len(self.keyed_vectors)\n",
    "    \n",
    "    pret_embedding = {}\n",
    "    embedding_matrix = np.zeros((n_words, emb_size))\n",
    "    count = 0\n",
    "\n",
    "    # Dicionário com todos os ids e palavras do embedding pré-treinado\n",
    "    # for index, word in enumerate(vocab.index_to_key):\n",
    "    for index, word in enumerate(self.keys_to_index.keys()):\n",
    "      pret_embedding[word] = index\n",
    "\n",
    "    # Construindo a embedding_matrix do embedding pré-treinado\n",
    "    for item in pret_embedding.items():\n",
    "      if item[1] < n_words:\n",
    "        count += 1\n",
    "        embedding_vector = self.keys_to_index[item[0]]\n",
    "        if embedding_vector is not None:\n",
    "          embedding_matrix[item[1]] = embedding_vector\n",
    "      \n",
    "    return embedding_matrix\n",
    "\n",
    "  def get_x_representation(self, tokens):\n",
    "    list_x = []\n",
    "    for token in tokens:\n",
    "      if(token in self.keys_to_index):\n",
    "        list_x.append(self.keys_to_index[token])\n",
    "      \n",
    "    return list_x\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokens_with_good_representation(df):\n",
    "  return df.loc[df.apply(lambda x: (len(x['tokens']) == len(x['word_embeddings'])) , axis=1)]\n",
    "\n",
    "\n",
    "def get_tokens_not_present_in_vocabullary(df, vocabulary):\n",
    "  tokens_with_bad_representation = df[df.apply(lambda x: (len(x['tokens']) != len(x['word_embeddings'])) , axis=1)]\n",
    "\n",
    "  tokens_not_present_in_vocab = []\n",
    "  for row_index, row in tokens_with_bad_representation.iterrows():\n",
    "    for token in row['tokens']:\n",
    "      if token not in vocabulary.keyed_vectors.keys():\n",
    "        tokens_not_present_in_vocab.append(token)\n",
    "\n",
    "  return set(tokens_not_present_in_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X, Y, embedding_matrix):\n",
    "  vocabulary_lenth, embedding_dimensionality = embedding_matrix.shape\n",
    "  \n",
    "  model = Sequential()\n",
    "  model.add(Embedding(vocabulary_lenth, embedding_dimensionality, input_length=X.shape[1], weights=[embedding_matrix]))\n",
    "  model.add(Bidirectional(LSTM(64)))\n",
    "  model.add(Dropout(0.1))\n",
    "  model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_train_and_test(embedding_name):\n",
    "  print('Reading file')\n",
    "  df_original = fm.read_annotated_df_with_embeddings(embedding_name)\n",
    "\n",
    "  count_before = df_original.txt.count()\n",
    "  print('Applying filter')\n",
    "  df = filter_tokens_with_good_representation(df_original)\n",
    "\n",
    "  rows_delted = count_before - df.txt.count()\n",
    "  if rows_delted:\n",
    "    print(f'There was deleted {rows_delted} rows with bad representation')\n",
    "\n",
    "  print('building vocabulary')\n",
    "  vocabulary = build_vocabulary_from_df(df)\n",
    "\n",
    "  tokens_not_present_in_vocab = get_tokens_not_present_in_vocabullary(df_original, vocabulary)\n",
    "  if tokens_not_present_in_vocab:\n",
    "    print(f'There are {len(tokens_not_present_in_vocab)} tokens not present in the vocabulary')\n",
    "\n",
    "  print('Creating data for train and test')\n",
    "  tokens_max_len = df['tokens'].apply(lambda x : len(x)).max()\n",
    "  \n",
    "  # df['token_indexes'] = df['tokens'].apply(vocabulary.get_x_representation)\n",
    "  df['token_indexes'] = df.loc[:, 'tokens'].apply(vocabulary.get_x_representation)\n",
    "\n",
    "  X = pad_sequences(maxlen=tokens_max_len, sequences=df['token_indexes'], value=0, padding='post', truncating='post')\n",
    "  Y = pd.get_dummies(df['intent']).values\n",
    "\n",
    "  return vocabulary, X, Y\n",
    "\n",
    "\n",
    "def train_model(embedding_name, vocabulary, X, Y):\n",
    "  print('Building model')\n",
    "  model = build_model(X, Y, vocabulary.embedding_matrix)\n",
    "  model.summary()\n",
    "\n",
    "  X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42,stratify=Y)\n",
    "\n",
    "  print('Trainning model')\n",
    "  history = model.fit(X_train, Y_train, epochs=20, batch_size=64,validation_split=0.1, \n",
    "                      callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n",
    "  print('Saving model')\n",
    "  path_model = fm.filename_from_data_dir(f'output/patient/lstm_models/{embedding_name}.h5')\n",
    "  model.save(path_model)\n",
    "\n",
    "  print('Evaluating the model')\n",
    "  accuracy = model.evaluate(X_test,Y_test)\n",
    "  print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accuracy[0],accuracy[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file\n",
      "Applying filter\n",
      "There was deleted 21 rows with bad representation\n",
      "building vocabulary\n",
      "There are 84 tokens not present in the vocabulary\n",
      "Creating data for train and test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_101806/808481887.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['token_indexes'] = df.loc[:, 'tokens'].apply(vocabulary.get_x_representation)\n"
     ]
    }
   ],
   "source": [
    "embedding_name = 'bert_pt'\n",
    "\n",
    "vocabulary, X, Y = prepare_data_for_train_and_test(embedding_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-16 17:54:29.560355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-16 17:54:29.561100: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-16 17:54:29.561272: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-16 17:54:29.561397: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-16 17:54:29.561581: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-06-16 17:54:29.561697: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-06-16 17:54:29.561736: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-16 17:54:29.561867: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-16 17:54:29.562033: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-06-16 17:54:29.562060: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-06-16 17:54:29.563803: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 103, 768)          4785408   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              426496    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,212,420\n",
      "Trainable params: 5,212,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Trainning model\n",
      "Epoch 1/20\n",
      "100/100 [==============================] - 14s 127ms/step - loss: 1.0956 - accuracy: 0.6136 - val_loss: 1.0329 - val_accuracy: 0.6374\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 12s 123ms/step - loss: 1.0527 - accuracy: 0.6191 - val_loss: 1.0186 - val_accuracy: 0.6374\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 12s 122ms/step - loss: 1.0424 - accuracy: 0.6190 - val_loss: 1.0107 - val_accuracy: 0.6374\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 1.0369 - accuracy: 0.6183 - val_loss: 1.0133 - val_accuracy: 0.6374\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 12s 122ms/step - loss: 1.0282 - accuracy: 0.6193 - val_loss: 1.0078 - val_accuracy: 0.6374\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 1.0240 - accuracy: 0.6182 - val_loss: 1.0011 - val_accuracy: 0.6374\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 12s 122ms/step - loss: 1.0144 - accuracy: 0.6198 - val_loss: 1.0055 - val_accuracy: 0.6374\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 12s 123ms/step - loss: 0.9923 - accuracy: 0.6176 - val_loss: 0.9916 - val_accuracy: 0.6105\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 12s 123ms/step - loss: 0.9867 - accuracy: 0.6160 - val_loss: 0.9890 - val_accuracy: 0.6374\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 13s 127ms/step - loss: 0.9882 - accuracy: 0.6160 - val_loss: 0.9817 - val_accuracy: 0.6374\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.9985 - accuracy: 0.6177 - val_loss: 0.9812 - val_accuracy: 0.6374\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 13s 127ms/step - loss: 0.9838 - accuracy: 0.6166 - val_loss: 0.9881 - val_accuracy: 0.6105\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 13s 127ms/step - loss: 0.9798 - accuracy: 0.6169 - val_loss: 0.9810 - val_accuracy: 0.6374\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 13s 127ms/step - loss: 0.9734 - accuracy: 0.6187 - val_loss: 0.9926 - val_accuracy: 0.6374\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 13s 127ms/step - loss: 0.9735 - accuracy: 0.6174 - val_loss: 0.9892 - val_accuracy: 0.6374\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 13s 127ms/step - loss: 0.9800 - accuracy: 0.6196 - val_loss: 0.9780 - val_accuracy: 0.6303\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 13s 128ms/step - loss: 0.9738 - accuracy: 0.6152 - val_loss: 0.9848 - val_accuracy: 0.6303\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.9740 - accuracy: 0.6183 - val_loss: 0.9839 - val_accuracy: 0.6105\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 14s 143ms/step - loss: 0.9720 - accuracy: 0.6201 - val_loss: 0.9785 - val_accuracy: 0.6105\n",
      "Saving model\n",
      "Evaluating the model\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 0.9621 - accuracy: 0.6117\n",
      "Test set\n",
      "  Loss: 0.962\n",
      "  Accuracy: 0.612\n"
     ]
    }
   ],
   "source": [
    "train_model(embedding_name, vocabulary, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file\n",
      "Applying filter\n",
      "building vocabulary\n",
      "Creating data for train and test\n"
     ]
    }
   ],
   "source": [
    "embedding_name = 'glove'\n",
    "\n",
    "vocabulary, X, Y = prepare_data_for_train_and_test(embedding_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 155, 300)          2145000   \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 128)              186880    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,332,396\n",
      "Trainable params: 2,332,396\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Trainning model\n",
      "Epoch 1/20\n",
      "101/101 [==============================] - 12s 102ms/step - loss: 0.7400 - accuracy: 0.7815 - val_loss: 0.6783 - val_accuracy: 0.8132\n",
      "Epoch 2/20\n",
      "101/101 [==============================] - 10s 95ms/step - loss: 0.6776 - accuracy: 0.7984 - val_loss: 0.5967 - val_accuracy: 0.8132\n",
      "Epoch 3/20\n",
      "101/101 [==============================] - 10s 96ms/step - loss: 0.6327 - accuracy: 0.7978 - val_loss: 0.5909 - val_accuracy: 0.8132\n",
      "Epoch 4/20\n",
      "101/101 [==============================] - 10s 96ms/step - loss: 0.6290 - accuracy: 0.7979 - val_loss: 0.5888 - val_accuracy: 0.8132\n",
      "Epoch 5/20\n",
      "101/101 [==============================] - 10s 97ms/step - loss: 0.6254 - accuracy: 0.7984 - val_loss: 0.5771 - val_accuracy: 0.8132\n",
      "Epoch 6/20\n",
      "101/101 [==============================] - 10s 96ms/step - loss: 0.6242 - accuracy: 0.7978 - val_loss: 0.5724 - val_accuracy: 0.8132\n",
      "Epoch 7/20\n",
      "101/101 [==============================] - 10s 96ms/step - loss: 0.6418 - accuracy: 0.7979 - val_loss: 0.5942 - val_accuracy: 0.8132\n",
      "Epoch 8/20\n",
      "101/101 [==============================] - 10s 96ms/step - loss: 0.6298 - accuracy: 0.7990 - val_loss: 0.5736 - val_accuracy: 0.8132\n",
      "Epoch 9/20\n",
      "101/101 [==============================] - 10s 99ms/step - loss: 0.6149 - accuracy: 0.8027 - val_loss: 0.5638 - val_accuracy: 0.8146\n",
      "Epoch 10/20\n",
      "101/101 [==============================] - 10s 99ms/step - loss: 0.6005 - accuracy: 0.8074 - val_loss: 0.5580 - val_accuracy: 0.8287\n",
      "Epoch 11/20\n",
      "101/101 [==============================] - 10s 95ms/step - loss: 0.5975 - accuracy: 0.8095 - val_loss: 0.5489 - val_accuracy: 0.8272\n",
      "Epoch 12/20\n",
      "101/101 [==============================] - 10s 94ms/step - loss: 0.5835 - accuracy: 0.8132 - val_loss: 0.5441 - val_accuracy: 0.8287\n",
      "Epoch 13/20\n",
      "101/101 [==============================] - 10s 96ms/step - loss: 0.6191 - accuracy: 0.8085 - val_loss: 0.5793 - val_accuracy: 0.8287\n",
      "Epoch 14/20\n",
      "101/101 [==============================] - 10s 94ms/step - loss: 0.6466 - accuracy: 0.8066 - val_loss: 0.6148 - val_accuracy: 0.8244\n",
      "Epoch 15/20\n",
      "101/101 [==============================] - 10s 94ms/step - loss: 0.6275 - accuracy: 0.8095 - val_loss: 0.5729 - val_accuracy: 0.8272\n",
      "Saving model\n",
      "Evaluating the model\n",
      "56/56 [==============================] - 1s 15ms/step - loss: 0.5974 - accuracy: 0.8169\n",
      "Test set\n",
      "  Loss: 0.597\n",
      "  Accuracy: 0.817\n"
     ]
    }
   ],
   "source": [
    "train_model(embedding_name, vocabulary, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "4l4iZ8z9gFgF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#model.load_weights(path_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96_fNT_WQ4Ue",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "5OTWQmkaQgY7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def print_pred_real_label(index):\n",
    "  #new_narrative = [train_df['RELATO'][index]]\n",
    "  #seq = tokenizer.texts_to_sequences(new_narrative)\n",
    "  #padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "  pred = model.predict(X[index-1:index])\n",
    "  labels = df['intent'].to_list()\n",
    "  print(np.argmax(pred))\n",
    "  print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2611243e-05, 3.7041116e-06, 9.9998200e-01, 1.6522043e-06]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 10\n",
    "\n",
    "pred = model.predict(X[index-1:index])\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7150, 300)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8900, 155)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[Pará State]Text_Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
