{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zL6SjRcWyrkQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-19 17:26:06.645515: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-19 17:26:06.645532: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from src.core import file_manager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_annotated_df_with_old_embeddings(embedding_name, actor='patient', variation='without_others_intent/k100_without_sentences_higher_than_median'):\n",
    "#     df = fm.read_json_of_dir(\n",
    "#         fm.filename_from_data_dir(\n",
    "#             f'embeddings/{embedding_name}/text_emb_{actor}.json.old'),\n",
    "#         lines=True\n",
    "#     )\n",
    "\n",
    "#     file_name = fm.filename_from_data_dir(\n",
    "#         f'output/{actor}/{variation}/{embedding_name}/annotated_sentences.csv'\n",
    "#     )\n",
    "\n",
    "#     df_annotated = pd.read_csv(file_name)\n",
    "\n",
    "#     df_merged = pd.merge(df_annotated, df, on='txt', how='left')\n",
    "\n",
    "#     return df_merged\n",
    "\n",
    "\n",
    "# df_old = read_annotated_df_with_old_embeddings('glove', variation='k100')\n",
    "# df = fm.read_annotated_df_with_embeddings('glove', variation='k100')\n",
    "# df['embeddings'].equals(df_old['embeddings'])\n",
    "\n",
    "# df_old = read_annotated_df_with_old_embeddings('bert_pt', variation='k100')\n",
    "# df = fm.read_annotated_df_with_embeddings('bert_pt', variation='k100')\n",
    "# df['embeddings'].equals(df_old['embeddings'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrUF3yEQzc7z",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fm.read_annotated_df_with_embeddings('glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7150\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "  for index, token in enumerate(row['tokens']):\n",
    "    if token not in vocab:\n",
    "      vocab[token] = row['word_embeddings'][index]\n",
    "\n",
    "n_source_words = len(vocab)\n",
    "\n",
    "print(n_source_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inform_symptoms    7120\n",
       "greeting            948\n",
       "request_inform      492\n",
       "inform_medicine     340\n",
       "Name: intent, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['intent'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "xptKqUwex1j2",
    "outputId": "0d63258f-2de3-4411-db43-503f87d5986e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\n"
     ]
    }
   ],
   "source": [
    "max_len = df['tokens'].apply(lambda x : len(x)).max()\n",
    "print(max_len)\n",
    "# max_len = 700 #75% do dataset pelo menos tem no maximo esse valor de tamanho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0GiyMsQejy3G",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(vocab, n_words, emb_size):\n",
    "    \n",
    "    pret_embedding = {}\n",
    "    embedding_matrix = np.zeros((n_words, emb_size))\n",
    "    count = 0\n",
    "\n",
    "    # Dicionário com todos os ids e palavras do embedding pré-treinado\n",
    "    # for index, word in enumerate(vocab.index_to_key):\n",
    "    for index, word in enumerate(vocab.keys()):\n",
    "      pret_embedding[word] = index\n",
    "\n",
    "    # Construindo a embedding_matrix do embedding pré-treinado\n",
    "    for item in pret_embedding.items():\n",
    "      if item[1] < n_words:\n",
    "        count += 1\n",
    "        embedding_vector = vocab[item[0]]\n",
    "        if embedding_vector is not None:\n",
    "          embedding_matrix[item[1]] = embedding_vector\n",
    "      \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GmIJNhfYDH8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Save the vectors  in a new Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "1lPLrypQaxMd",
    "outputId": "f85dd86b-ff35-44bb-8c49-582c6bbe0f50",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7150, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_size = 300\n",
    "embedding_size = len(df.loc[0].word_embeddings[0])\n",
    "embedding_matrix = create_embedding_matrix(vocab, n_source_words, embedding_size)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_key_2_index =  {key: index for index, key in enumerate(vocab.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, dict, dict)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embedding_matrix), type(vocab), type(vocab_key_2_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umj8ZOBwbT4C",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que cria o X\n",
    "def create_x(tokens):\n",
    "    list_x = []\n",
    "    for token in tokens:\n",
    "      if(token in vocab_key_2_index):\n",
    "        list_x.append(vocab_key_2_index[token])\n",
    "      \n",
    "    return list_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>annotated_txt</th>\n",
       "      <th>intent</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word_embeddings</th>\n",
       "      <th>token_indexes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Com muito medo</td>\n",
       "      <td>Com muito medo</td>\n",
       "      <td>inform_symptoms</td>\n",
       "      <td>[[0.07444667, -0.04100433, -0.39161667, 0.1238...</td>\n",
       "      <td>[Com, muito, medo]</td>\n",
       "      <td>[[0.039064, 0.34595000000000004, 0.11547, -0.4...</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tá  certo</td>\n",
       "      <td>Tá  certo</td>\n",
       "      <td>greeting</td>\n",
       "      <td>[[-0.0368989, 0.004874997000000001, 0.14075000...</td>\n",
       "      <td>[Tá, certo]</td>\n",
       "      <td>[[-0.0089978, -0.15736, 0.57216, 0.49819, -0.2...</td>\n",
       "      <td>[3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tá  bom</td>\n",
       "      <td>Tá  bom</td>\n",
       "      <td>greeting</td>\n",
       "      <td>[[0.09625109999999999, -0.21625000000000003, 0...</td>\n",
       "      <td>[Tá, bom]</td>\n",
       "      <td>[[-0.0089978, -0.15736, 0.57216, 0.49819, -0.2...</td>\n",
       "      <td>[3, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boa tarde! Tenho uma idosa de 84 anos e vivi n...</td>\n",
       "      <td>Boa tarde! Tenho uma idosa de 84 anos e vivi n...</td>\n",
       "      <td>inform_symptoms</td>\n",
       "      <td>[[0.008652593, -0.048798796000000005, -0.15833...</td>\n",
       "      <td>[Boa, tarde, !, Tenho, uma, idosa, de, 84, ano...</td>\n",
       "      <td>[[0.077468, -0.15121, -0.5625300000000001, 0.4...</td>\n",
       "      <td>[6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acabei de enviar</td>\n",
       "      <td>Acabei de enviar</td>\n",
       "      <td>inform_symptoms</td>\n",
       "      <td>[[0.07401667, -0.06999668, -0.2369833400000000...</td>\n",
       "      <td>[Acabei, de, enviar]</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[93, 12, 94]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 txt  \\\n",
       "0                                     Com muito medo   \n",
       "1                                          Tá  certo   \n",
       "2                                            Tá  bom   \n",
       "3  Boa tarde! Tenho uma idosa de 84 anos e vivi n...   \n",
       "4                                   Acabei de enviar   \n",
       "\n",
       "                                       annotated_txt           intent  \\\n",
       "0                                     Com muito medo  inform_symptoms   \n",
       "1                                          Tá  certo         greeting   \n",
       "2                                            Tá  bom         greeting   \n",
       "3  Boa tarde! Tenho uma idosa de 84 anos e vivi n...  inform_symptoms   \n",
       "4                                   Acabei de enviar  inform_symptoms   \n",
       "\n",
       "                                          embeddings  \\\n",
       "0  [[0.07444667, -0.04100433, -0.39161667, 0.1238...   \n",
       "1  [[-0.0368989, 0.004874997000000001, 0.14075000...   \n",
       "2  [[0.09625109999999999, -0.21625000000000003, 0...   \n",
       "3  [[0.008652593, -0.048798796000000005, -0.15833...   \n",
       "4  [[0.07401667, -0.06999668, -0.2369833400000000...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                                 [Com, muito, medo]   \n",
       "1                                        [Tá, certo]   \n",
       "2                                          [Tá, bom]   \n",
       "3  [Boa, tarde, !, Tenho, uma, idosa, de, 84, ano...   \n",
       "4                               [Acabei, de, enviar]   \n",
       "\n",
       "                                     word_embeddings  \\\n",
       "0  [[0.039064, 0.34595000000000004, 0.11547, -0.4...   \n",
       "1  [[-0.0089978, -0.15736, 0.57216, 0.49819, -0.2...   \n",
       "2  [[-0.0089978, -0.15736, 0.57216, 0.49819, -0.2...   \n",
       "3  [[0.077468, -0.15121, -0.5625300000000001, 0.4...   \n",
       "4  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                       token_indexes  \n",
       "0                                          [0, 1, 2]  \n",
       "1                                             [3, 4]  \n",
       "2                                             [3, 5]  \n",
       "3  [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1...  \n",
       "4                                       [93, 12, 94]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['token_indexes'] = df['tokens'].apply(lambda tokens : create_x(tokens))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8900, 155), (8900, 4))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pad_sequences(maxlen=max_len, sequences=df['token_indexes'], value=0, padding='post', truncating='post')\n",
    "Y = pd.get_dummies(df['intent']).values\n",
    "(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NDzsrjXk4sU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "pbaMEtTVk67l",
    "outputId": "8d3fe25a-2041-4ac9-a788-2cf07a156f8a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7120, 155) (7120, 4)\n",
      "(1780, 155) (1780, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42,stratify=Y)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjRDbCNoPaoz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "M_iUMZidPGBV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-19 17:26:16.990687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-19 17:26:16.991034: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-19 17:26:16.991066: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-19 17:26:16.991092: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-19 17:26:16.991120: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-06-19 17:26:16.991146: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-06-19 17:26:16.991171: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-19 17:26:16.991195: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-19 17:26:16.991220: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-06-19 17:26:16.991225: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-06-19 17:26:16.991453: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "num_labels = Y.shape[1]\n",
    "MAX_NB_WORDS = n_source_words\n",
    "# EMBEDDING_DIM = 50\n",
    "EMBEDDING_DIM = embedding_size\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1], weights=[embedding_matrix]))\n",
    "# model.add(Embedding(X.shape[0], X.shape[1], input_length=X.shape[1], weights=[X]))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(num_labels, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#Optimisation functions usually calculate the gradient i.e. the partial derivative of loss function with respect to weights, \n",
    "#and the weights are modified in the opposite direction of the calculated gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "iaMmhU8a47I6",
    "outputId": "bfc5473a-d5ab-4080-aad2-e56e49b7712e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 155, 300)          2145000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              186880    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,332,396\n",
      "Trainable params: 2,332,396\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "id": "aWhGma7KPe5Q",
    "outputId": "59c72d10-a8e1-444c-e125-2f8dacfbb791",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "101/101 [==============================] - 10s 87ms/step - loss: 0.3964 - accuracy: 0.8663 - val_loss: 0.1653 - val_accuracy: 0.9522\n",
      "Epoch 2/20\n",
      "101/101 [==============================] - 8s 83ms/step - loss: 0.1094 - accuracy: 0.9677 - val_loss: 0.0980 - val_accuracy: 0.9691\n",
      "Epoch 3/20\n",
      "101/101 [==============================] - 8s 83ms/step - loss: 0.0627 - accuracy: 0.9803 - val_loss: 0.0845 - val_accuracy: 0.9747\n",
      "Epoch 4/20\n",
      "101/101 [==============================] - 8s 83ms/step - loss: 0.0356 - accuracy: 0.9885 - val_loss: 0.0852 - val_accuracy: 0.9747\n",
      "Epoch 5/20\n",
      "101/101 [==============================] - 8s 83ms/step - loss: 0.0223 - accuracy: 0.9939 - val_loss: 0.1218 - val_accuracy: 0.9705\n",
      "Epoch 6/20\n",
      "101/101 [==============================] - 9s 84ms/step - loss: 0.0154 - accuracy: 0.9966 - val_loss: 0.1151 - val_accuracy: 0.9705\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1, \n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model the model and vocabullary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = fm.filename_from_data_dir(f'output/lstm_models/patient/glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "uRrwj82JYEP6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the model\n"
     ]
    }
   ],
   "source": [
    "print('saving the model')\n",
    "model.save(f'{working_dir}/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the vocabullary\n"
     ]
    }
   ],
   "source": [
    "print('saving the vocabullary')\n",
    "\n",
    "file = open(f'{working_dir}/vocabullary.json',\"w\")\n",
    "\n",
    "file.write(json.dumps(vocab_key_2_index))\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the metadata\n"
     ]
    }
   ],
   "source": [
    "print('saving the metadata')\n",
    "\n",
    "file = open(f'{working_dir}/metadata.json',\"w\")\n",
    "\n",
    "intents = pd.get_dummies(df['intent']).columns.tolist()\n",
    "\n",
    "metadata = {\n",
    "  'intents': intents,\n",
    "  'vector_length': str(max_len)\n",
    "}\n",
    "\n",
    "file.write(json.dumps(metadata))\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OtcdG7lQKWe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "UAW8hzQWQA_6",
    "outputId": "df64dc70-e57d-464e-877a-8c6c78f75bf8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 1s 14ms/step - loss: 0.1436 - accuracy: 0.9618\n",
      "Test set\n",
      "  Loss: 0.144\n",
      "  Accuracy: 0.962\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[Pará State]Text_Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
